---
title: "What is Data Science?"
format: html
---

Data science is often described as a combination of mathematics, statistics, programming, advanced analytics, artificial intelligence, and machine learning, combined with subject matter expertise to uncover actionable insights.

While this definition is technically accurate, it is not always helpful.

For most people working in education, healthcare, or research, the more useful question is not: **“What is data science?”**

But rather:\
**“What kinds of skills are needed to work with data well?”**

## Data Skill Set Definitions

Data science is a broad ecosystem of skills. No one person has all of them, and most roles overlap in practice. It is helpful to think in terms of types of work, not job titles.

|  |  |  |
|------------------------|------------------------|------------------------|
| **Data Engineering:** | **Data Science:** | **Data Analysis:** |
| This work focuses on how data are stored, structured, and maintained. It includes managing hardware and storage systems, as well as writing code to handle messy or unstructured data. | This work often focuses on software development, large-scale datasets, and cloud-based systems. It frequently involves advanced modeling and machine learning. | **This is where many educators and researchers already operate.** It includes cleaning and organizing data, analyzing patterns and outcomes, creating visualizations, and communicating results clearly in writing and presentations. |

## "Big Data" is not the point

The term “big data” is often used to describe massive datasets such as search engine queries or real-time commercial analytics. Most people working in academic, educational, or clinical settings will never interact with data at this scale.

However, data science tools are still valuable even when datasets are small.

Even if all your data fits on a single screen, modern data workflows can reduce errors, improve consistency, make analyses faster, and allow others to understand what was done.

## The Data Ecosystem Problem

Data rarely live in one place forever. Over the life of a project, data may be collected in one system, cleaned in another, analyzed in multiple versions, revised in response to feedback, and shared with collaborators or reviewers.

Each transition creates opportunities for data loss, version confusion, and error.

Reproducible workflows help by creating a clear, documented path from raw data to final results.

## Reproducibility: Controlling What We Can

Reproducibility means that the same input data and the same analysis steps produce the same output every time.

In practice, this means **data are preserved in their original form**, analysis steps are documented, and results can be regenerated by re-running the analysis.

While we cannot control all sources of variation, we can control whether our workflows are reproducible.

**WHAT THIS MEANS FOR EXCEL USERS:**\
Excel is a powerful and familiar tool, and it often plays an important role in data workflows. Moving toward reproducible workflows does not mean abandoning Excel immediately.

Instead, it means being intentional about how data are entered and edited, reducing manual undocumented steps, creating clear transitions between data cleaning, analysis, and reporting, and using tools that allow analyses to be repeated without starting over.

## Basic concepts & terminology

::: panel-tabset
## Data Frame

A data frame is a table of data where each row represents an observation (such as a student, course, or exam) and each column represents a variable. Data frames are the primary way data are organized in R and are similar in structure to Excel spreadsheets when those spreadsheets are used consistently.

However, many common Excel formatting practices prevent a sheet from functioning as a true data frame. Merged cells, blank rows or columns, embedded totals, and missing values used for visual spacing all break the row-by-column structure that data analysis tools rely on. While these formats may look clear to a human reader, they make it difficult for software to reliably interpret the data.

This guide emphasizes structuring data so that it behaves as a data frame, even when it is created or viewed in Excel.

## Variable

A variable is a single column in a dataset that contains a specific type of information, such as a score, term, pathway, or outcome.

## Data labels

Data labels are names or categories used to describe variables or values within a dataset. Clear, consistent labels make it easier to understand what the data represent and reduce the risk of misinterpretation during analysis.

In more complex projects, data labels are often supported by a data dictionary. A data dictionary is a separate document that defines each variable, explains how values are coded, and provides context for how the data were collected or generated. While data dictionaries are not required for every project, they are especially helpful when datasets are shared, revisited over time, or used by multiple people.

This guide introduces simple labeling practices and shows how even lightweight documentation can improve clarity and reproducibility.

## Logic

Logic refers to the rules used to make decisions within data analysis. These rules determine which data are included, excluded, grouped, or summarized based on specific conditions.

In academic program and assessment work, logic is often used to answer questions such as which students belong to a specific cohort, which exam scores fall within a given term, or which outcomes meet a defined threshold. For example, logic can be used to select exam grades from a particular term, group those grades by cohort, and then summarize or visualize the results in a chart.

Using explicit logical rules in code makes these decisions transparent and repeatable. Instead of manually filtering or sorting data, the logic used to create a figure or summary is documented and can be applied consistently across terms or updated datasets.

## Code

Code is a written set of instructions that tells a computer exactly what steps to perform on data. These instructions can include importing data, selecting specific values, performing calculations, creating summaries, and generating visualizations.

There are many programming languages used for data analysis, each with different strengths. Common examples include R, Python, and SQL. No single language is “best” in all situations, and learning one language makes it easier to learn others over time.

For the purposes of this guide, R is used because it is free, widely used in academic and research settings, and well suited for data analysis and visualization. R allows analysis steps to be written clearly and run again on updated data, supporting reproducible workflows without requiring advanced programming experience.

This guide focuses on using small, readable pieces of code to document data analysis decisions rather than on complex or highly technical programming.

## Reproducibility

Reproducibility means that the same data and the same analysis steps produce the same results every time the analysis is run. In a reproducible workflow, data are preserved in their original form, analysis steps are documented using code, and results can be regenerated simply by re-running the analysis.

For academic programs that review data term-by-term or annually, reproducibility is especially powerful because it improves efficiency over time. Once an analysis workflow is written, the same code can be reused with new data each term or year, eliminating the need to rebuild tables, charts, or reports from scratch. This reduces manual effort, minimizes errors introduced by repeated copying or filtering, and ensures that comparisons across time are based on consistent methods.

Reproducible workflows also make it easier to respond to new questions or reporting requests. Because the logic of the analysis is explicit, adjustments can be made quickly without undoing prior work. Over time, this allows programs to shift from reactive data processing to more intentional, longitudinal analysis.
:::
